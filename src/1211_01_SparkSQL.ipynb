{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425df4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b9954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 17:11:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/12 17:11:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/12 17:11:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "# User 데이터 정의\n",
    "user_data = [\n",
    "    Row(user_id=1, username='A', address='서울'),\n",
    "    Row(user_id=2, username='B', address='대전'),\n",
    "    Row(user_id=3, username='C', address='경기도'),\n",
    "    Row(user_id=4, username='D', address=None),\n",
    "    Row(user_id=5, username='E', address=None),\n",
    "    Row(user_id=6, username='F', address='서울'),\n",
    "    Row(user_id=7, username='G', address='경기도'),\n",
    "    Row(user_id=8, username='H', address='대구'),\n",
    "    Row(user_id=9, username='I', address='부산'),\n",
    "    Row(user_id=10, username='J', address='전주'),\n",
    "    Row(user_id=11, username='K', address='광주')\n",
    "]\n",
    "\n",
    "# Books 데이터 정의\n",
    "books_data = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35)\n",
    "]\n",
    "\n",
    "# User 데이터프레임 생성\n",
    "user_df = spark.createDataFrame(user_data)\n",
    "\n",
    "user_df.createOrReplaceTempView('users') # 테이블로 보이기  # SQL 사용을 위해 TempView 등록\n",
    "\n",
    "# Books 데이터프레임 생성\n",
    "books_df = spark.createDataFrame(books_data)\n",
    "books_df.createOrReplaceTempView('books') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fee8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|address| address|\n",
      "+-------+--------+\n",
      "|   서울|    서울|\n",
      "|   대전|    대전|\n",
      "| 경기도|  경기도|\n",
      "|   null|주소없음|\n",
      "|   null|주소없음|\n",
      "|   서울|    서울|\n",
      "| 경기도|  경기도|\n",
      "|   대구|    대구|\n",
      "|   부산|    부산|\n",
      "|   전주|    전주|\n",
      "|   광주|    광주|\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_users='''\n",
    "select address,\n",
    "    IF(address IS NULL, '주소없음', address) AS address\n",
    "FROM users;\n",
    "\n",
    "'''\n",
    "spark.sql(query_users).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4479de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books table\n",
    "#stock_quantity >=50 '재고 많음', >= 30 '재고 중간', 재고 없음 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5f674b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|stock_quantity|quantity_level|\n",
      "+--------------+--------------+\n",
      "|            55|     재고 많음|\n",
      "|            40|     재고 중간|\n",
      "|            20|     재고 없음|\n",
      "|            75|     재고 많음|\n",
      "|            35|     재고 중간|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql = '''\n",
    "SELECT stock_quantity, \n",
    "\t   IF(stock_quantity >= 50, '재고 많음',\n",
    "\t\t  IF(stock_quantity >= 30, '재고 중간', '재고 없음')) AS quantity_level\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(books_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c4c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|stock_quantity|quantity_level|\n",
      "+--------------+--------------+\n",
      "|            55|     재고 많음|\n",
      "|            40|     재고 중간|\n",
      "|            20|     재고 부족|\n",
      "|            75|     재고 많음|\n",
      "|            35|     재고 중간|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_1= '''\n",
    "SELECT stock_quantity, \n",
    "\t   CASE \n",
    "\t\t   WHEN stock_quantity >= 50 THEN '재고 많음'\n",
    "\t\t   WHEN stock_quantity >= 30 THEN '재고 중간'\n",
    "\t\t   ELSE '재고 부족'\n",
    "\t   END AS quantity_level\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(books_sql_1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ded780",
   "metadata": {},
   "source": [
    "# 실행계획 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8dba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [stock_quantity#77L, if ((stock_quantity#77L >= 50)) 재고 많음 else if ((stock_quantity#77L >= 30)) 재고 중간 else 재고 없음 AS quantity_level#145]\n",
      "+- *(1) Scan ExistingRDD[book_id#71L,title#72,author_fname#73,author_lname#74,pages#75L,released_year#76L,stock_quantity#77L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7a63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [stock_quantity#77L, CASE WHEN (stock_quantity#77L >= 50) THEN 재고 많음 WHEN (stock_quantity#77L >= 30) THEN 재고 중간 ELSE 재고 부족 END AS quantity_level#148]\n",
      "+- *(1) Scan ExistingRDD[book_id#71L,title#72,author_fname#73,author_lname#74,pages#75L,released_year#76L,stock_quantity#77L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql_1).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2561393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#74], functions=[])\n",
      "+- Exchange hashpartitioning(author_lname#74, 200), ENSURE_REQUIREMENTS, [id=#117]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#74], functions=[])\n",
      "      +- *(1) Project [author_lname#74]\n",
      "         +- *(1) Scan ExistingRDD[book_id#71L,title#72,author_fname#73,author_lname#74,pages#75L,released_year#76L,stock_quantity#77L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_2 = '''\n",
    "select distinct author_lname from books;\n",
    "'''\n",
    "spark.sql(books_sql_2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b224fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:========================================>              (73 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|author_lname|\n",
      "+------------+\n",
      "|       Jones|\n",
      "|       Davis|\n",
      "|       Smith|\n",
      "|         Doe|\n",
      "|       Brown|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd395e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#74], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(author_lname#74, 200), ENSURE_REQUIREMENTS, [id=#164]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#74], functions=[partial_count(1)])\n",
      "      +- *(1) Project [author_lname#74]\n",
      "         +- *(1) Scan ExistingRDD[book_id#71L,title#72,author_fname#73,author_lname#74,pages#75L,released_year#76L,stock_quantity#77L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_3 = '''\n",
    "select author_lname, count(*)\n",
    "from books\n",
    "group by author_lname;\n",
    "'''\n",
    "spark.sql(books_sql_3).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|author_lname|count(1)|\n",
      "+------------+--------+\n",
      "|       Jones|       1|\n",
      "|       Davis|       1|\n",
      "|       Smith|       1|\n",
      "|         Doe|       1|\n",
      "|       Brown|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql_3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6991ef8",
   "metadata": {},
   "source": [
    "#  데이터 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c5a2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books 테이블 데이터에 borrowed_by 추가\n",
    "books_data_with_user = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55, borrowed_by=1),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40, borrowed_by=2),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20, borrowed_by=3),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75, borrowed_by=None),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35, borrowed_by=6)\n",
    "]\n",
    "\n",
    "\n",
    "# DataFrame 생성\n",
    "books_df_with_user = spark.createDataFrame(books_data_with_user)\n",
    "\n",
    "# Temp View 등록\n",
    "books_df_with_user.createOrReplaceTempView(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d71942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            20|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql = '''\n",
    "SELECT *\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(books_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "967983fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            50|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "#book_id =3, stock_quantity= 50 으로 바꾼다. -> 전처리 과정\n",
    "updated_books_df= books_df_with_user.withColumn( \n",
    "    \"stock_quantity\",\n",
    "    when(books_df_with_user.book_id == 3, 50).otherwise(books_df_with_user.stock_quantity)\n",
    ")\n",
    "updated_books_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c58af33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            60|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            44|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            22|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            82|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            38|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stock_quantity * 10% 증가 \n",
    "\n",
    "updated_books_df= books_df_with_user.withColumn( \n",
    "    \"stock_quantity\",\n",
    "    (col(\"stock_quantity\")*1.1).cast(\"int\")\n",
    ")\n",
    "\n",
    "#뷰로 등록 \n",
    "updated_books_df.createOrReplaceTempView(\"books\")\n",
    "spark.sql(\"select* from books\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620052f",
   "metadata": {},
   "source": [
    "# 데이터 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "475b5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode : overwrite, append, ignore, error\n",
    "\n",
    "updated_books_df.write.csv(\"data/output/sqltest_updated_books.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51ddd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.write.csv(\"user\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc1689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_books_df1= spark.read.csv(\"data/output/sqltest_updated_books.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "346cc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            22|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            82|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            38|          6|\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            60|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            44|          2|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_books_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dfca413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 13:33:14 WARN ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n"
     ]
    }
   ],
   "source": [
    "user_df1= spark.read.csv(\"user\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c044ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533099f",
   "metadata": {},
   "source": [
    "# 조인 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81037f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:=====================================================> (98 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#book_id, author_fnaem, author_lname, username, address\n",
    "join_query= '''\n",
    "SELECT book_id,title, author_fname, author_lname, username, address\n",
    "FROM books b INNER JOIN users u ON b.borrowed_by = u.user_id;\n",
    "'''\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#books LEFT JOIN users \n",
    "SELECT book_id,title, author_fname, author_lname, username, address\n",
    "FROM books b LEFT JOIN users u ON b.borrowed_by = u.user_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자의 책 대여 목록 > 전체 사용자 > 대여한 정보가 있으면 나오면, 없으면 NULL\n",
    "# books RIGHT JOIN users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1a87b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:====================================>                  (67 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|   null|  null|        null|        null|       G| 경기도|\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "|   null|  null|        null|        null|       I|   부산|\n",
      "|   null|  null|        null|        null|       E|   null|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|   null|  null|        null|        null|       J|   전주|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|   null|  null|        null|        null|       H|   대구|\n",
      "|   null|  null|        null|        null|       K|   광주|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "|   null|  null|        null|        null|       D|   null|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#user 기준으로 book 붙이기 \n",
    "\n",
    "join_query= '''\n",
    "SELECT book_id,title, author_fname, author_lname, username, address\n",
    "FROM users u LEFT JOIN books b ON u.user_id= b.borrowed_by;\n",
    "'''\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495539b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정지역= 서울에 거주하는 사용자가 대여한 책 목록 \n",
    "\n",
    "join_query= '''\n",
    "SELECT book_id,title, author_fname, author_lname, username, address\n",
    "FROM users u LEFT JOIN books b ON u.user_id= b.borrowed_by\n",
    "WHERE u.address = '서울';\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0600045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 89:=============================================>         (83 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|user_id|username|count(book_id)|\n",
      "+-------+--------+--------------+\n",
      "|      7|       G|             0|\n",
      "|      6|       F|             1|\n",
      "|      9|       I|             0|\n",
      "|      5|       E|             0|\n",
      "|      1|       A|             1|\n",
      "|     10|       J|             0|\n",
      "|      3|       C|             1|\n",
      "|      8|       H|             0|\n",
      "|     11|       K|             0|\n",
      "|      2|       B|             1|\n",
      "|      4|       D|             0|\n",
      "+-------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자별로 대여한 책 수\n",
    "join_query= '''\n",
    "SELECT user_id, username , count(book_id)\n",
    "FROM users u LEFT JOIN books b ON u.user_id= b.borrowed_by -- 실행은 ;  \n",
    "group by u.user_id, u.username;  \n",
    "'''\n",
    "spark.sql(join_query).show() # MYSQL에 입력하면 똑같은 결과값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d02429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-------------+\n",
      "|book_id| title|pages|book_category|\n",
      "+-------+------+-----+-------------+\n",
      "|      1|Book A|  300|         Long|\n",
      "|      2|Book B|  250|        Short|\n",
      "|      3|Book C|  180|        Short|\n",
      "|      4|Book D|  320|         Long|\n",
      "|      5|Book E|  270|        Short|\n",
      "+-------+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#book_category >300 이상이면 long, short 카테고리 추가 \n",
    "\n",
    "join_query= '''\n",
    "SELECT book_id,title,pages, CASE\n",
    "               WHEN pages>=300 THEN 'Long' ELSE 'Short'\n",
    "               END AS book_category \n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(join_query).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e739d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock_quantity >50 이상 '충분', 30 이상 '보통' , 미만 '부족'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d032944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+------------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|stock_status|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+------------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            60|          1|        충분|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            44|          2|        보통|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            22|          3|        부족|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            82|       null|        충분|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            38|          6|        보통|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_query= '''\n",
    "SELECT *, CASE \n",
    "               WHEN stock_quantity > 50 THEN '충분'\n",
    "               WHEN stock_quantity >= 30 THEN '보통'\n",
    "               ELSE '부족'\n",
    "           END AS stock_status\n",
    "    FROM books;\n",
    "'''\n",
    "spark.sql(join_query).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60872c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 책 제목에 특정 키워드가 포함되어 있는지 확인항 때\n",
    "'''WHERE title LIKE '%A%''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0484c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:===================================>                  (131 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+\n",
      "|author_fname|author_lname|borrow_count|\n",
      "+------------+------------+------------+\n",
      "|       Chris|       Brown|           1|\n",
      "|        Anna|       Davis|           1|\n",
      "|       Emily|       Jones|           1|\n",
      "|        Jane|       Smith|           1|\n",
      "|        John|         Doe|           1|\n",
      "+------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:==================================================>   (188 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 대여가 많이 된 책의 작가를 조회 \n",
    "join_query= '''\n",
    "    SELECT author_fname, author_lname, count(book_id) as borrow_count\n",
    "    FROM books \n",
    "    GROUP BY  author_fname, author_lname\n",
    "    ORDER BY borrow_count DESC ;\n",
    "'''\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f03926e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|released_year|borrowed_books_count|\n",
      "+-------------+--------------------+\n",
      "|         2005|                   1|\n",
      "|         2008|                   1|\n",
      "|         2010|                   1|\n",
      "|         2012|                   1|\n",
      "|         2015|                   1|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 100:==========================================>          (160 + 3) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#책의 발행 연도별 대여 현황: 발행 연도별로 대여된 책의 수를 확인합니다.\n",
    "join_query= '''\n",
    "SELECT released_year,\n",
    "       COUNT(*) AS borrowed_books_count\n",
    "FROM books\n",
    "GROUP BY released_year\n",
    "ORDER BY \n",
    "released_year;\n",
    "'''\n",
    "spark.sql(join_query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1b79b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:=============================================>       (173 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|address|borrowed_count|\n",
      "+-------+--------------+\n",
      "|   서울|             2|\n",
      "|   null|             1|\n",
      "|   대전|             1|\n",
      "| 경기도|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 104:==========================================>          (162 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 사용자의 지역별 대여된 책 수: 사용자 지역별로 대여된 책의 수를 계산합니다.\n",
    "join_query= '''\n",
    "SELECT u.address, COUNT(book_id) AS borrowed_count\n",
    "FROM books b LEFT JOIN  users u ON  b.borrowed_by = u.user_id\n",
    "GROUP BY u.address\n",
    "ORDER BY borrowed_count DESC; \n",
    "'''\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5a60a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------+------------+-----+\n",
      "|book_id|title|author_fname|author_lname|pages|\n",
      "+-------+-----+------------+------------+-----+\n",
      "+-------+-----+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대여되지 않은 책 중 가장 페이지 수가 많은 책: 대여되지 않은 책 중에서 페이지 수가 가장 많은 책을 조회합니다.\n",
    "join_query= '''\n",
    "SELECT book_id, title, author_fname, author_lname, pages\n",
    "FROM books\n",
    "WHERE borrowed_by = 0\n",
    "ORDER BY pages DESC\n",
    "LIMIT 1;\n",
    "'''\n",
    "spark.sql(join_query).show()\n",
    "# 다시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f530cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 재고가 부족한 책과 대여 상태: 재고가 30개 미만인 책과 해당 책이 대여된 상태인지 확인합니다.\n",
    "join_query= '''\n",
    "SELECT book_id, title, stock_quantity, CASE \n",
    "        WHEN borrowed_count > 0 THEN '대여됨'\n",
    "        ELSE '대여되지 않음'\n",
    "    END AS borrow_status\n",
    "FROM books b LEFT JOIN users u ON b.borrowed_by= u.user_id\n",
    "WHERE stock_quantity < 30;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a59f0185",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`borrowed_count`' given input columns: [u.address, b.author_fname, b.author_lname, b.book_id, b.borrowed_by, b.pages, b.released_year, b.stock_quantity, b.title, u.user_id, u.username]; line 3 pos 13;\n'Project [book_id#192L, title#193, stock_quantity#307, CASE WHEN ('borrowed_count > 0) THEN 대여됨 ELSE 대여되지 않음 END AS borrow_status#713]\n+- Filter (stock_quantity#307 < 30)\n   +- Join LeftOuter, (borrowed_by#199L = user_id#62L)\n      :- SubqueryAlias b\n      :  +- SubqueryAlias books\n      :     +- Project [book_id#192L, title#193, author_fname#194, author_lname#195, pages#196L, released_year#197L, cast((cast(stock_quantity#198L as double) * 1.1) as int) AS stock_quantity#307, borrowed_by#199L]\n      :        +- LogicalRDD [book_id#192L, title#193, author_fname#194, author_lname#195, pages#196L, released_year#197L, stock_quantity#198L, borrowed_by#199L], false\n      +- SubqueryAlias u\n         +- SubqueryAlias users\n            +- LogicalRDD [user_id#62L, username#63, address#64], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`borrowed_count`' given input columns: [u.address, b.author_fname, b.author_lname, b.book_id, b.borrowed_by, b.pages, b.released_year, b.stock_quantity, b.title, u.user_id, u.username]; line 3 pos 13;\n'Project [book_id#192L, title#193, stock_quantity#307, CASE WHEN ('borrowed_count > 0) THEN 대여됨 ELSE 대여되지 않음 END AS borrow_status#713]\n+- Filter (stock_quantity#307 < 30)\n   +- Join LeftOuter, (borrowed_by#199L = user_id#62L)\n      :- SubqueryAlias b\n      :  +- SubqueryAlias books\n      :     +- Project [book_id#192L, title#193, author_fname#194, author_lname#195, pages#196L, released_year#197L, cast((cast(stock_quantity#198L as double) * 1.1) as int) AS stock_quantity#307, borrowed_by#199L]\n      :        +- LogicalRDD [book_id#192L, title#193, author_fname#194, author_lname#195, pages#196L, released_year#197L, stock_quantity#198L, borrowed_by#199L], false\n      +- SubqueryAlias u\n         +- SubqueryAlias users\n            +- LogicalRDD [user_id#62L, username#63, address#64], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행계획, DAG 형태 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv 로 save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beccb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b4d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
