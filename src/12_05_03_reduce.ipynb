{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d21cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 10:50:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 10:50:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# 스파크 환경 설정 객체 생성\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"241205_03_reduce\")\n",
    "spark =SparkContext(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여러개의 값을 하나로 축약\n",
    "# RDD.reduce(<func>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fb0cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd= spark.parallelize([1,2,3,4,5])\n",
    "sample_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6579dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f49e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd.reduce(add) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffda62c",
   "metadata": {},
   "source": [
    "# partition 이 나누어져 있을 때 \n",
    "\n",
    "lambda x,y : (x*2) +y\n",
    "\n",
    "1: x=1, y=2 >> (1*2)+2 =4\n",
    "2: x=4, y=3  >> (4*2) +3=1\n",
    "3: x=11, y=4 >>(11*2) +4 =26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3b4147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd1= spark.parallelize([1,2,3,4]) #여기는 하나의 파티션 \n",
    "sample_rdd1.reduce(lambda x,y:(x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9aa25af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d678d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd2= spark.parallelize([1,2,3,4],2) #파티션 쪼개기 \n",
    "sample_rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8fd4073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd2.reduce(lambda x,y:(x*2)+y) #파티션 나눴을 떄 \n",
    "# 4 , 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e798a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파티션 3개일 때 - 설명 참고 \n",
    "\n",
    "# x: p1 y:p2 p3:x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d621f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3, 4]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파티션 3개일 때\n",
    "\n",
    "# x: p1 y:p2 p3:x,y \n",
    "sample_rdd3= spark.parallelize([1,2,3,4],3)\n",
    "sample_rdd3.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bf5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e936d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd3.reduce(lambda x,y:(x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eda438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98669fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#리듀스 연산은 순서 의존적 \n",
    "# 각 파티션 내에서 연산은 독립적으로 실행, 최종 결과는 파티션 간의 결과가 결합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f0681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170faa85",
   "metadata": {},
   "source": [
    "###fold  연산 활용 \n",
    "'''\n",
    "\n",
    "fold( zeroValue, <func> )\n",
    "    '''\n",
    "    zeroValue:시작값 , 항등원 * 일 때 1, + 일 경우0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4cbb40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4= spark.parallelize([2,3,4],4)  #비어있는 값없음 \n",
    "rdd4.reduce(lambda x,y:x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8c50eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.fold(1, lambda x,y:x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f37a8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce Error : Can not reduce() empty RDD\n"
     ]
    }
   ],
   "source": [
    " #오류상황 가정, 빈RDD 예외처리 \n",
    "rdd5=  spark.parallelize([])  \n",
    "try:\n",
    "    rdd5.reduce(lambda a,b:a+b)\n",
    "except ValueError as e:  \n",
    "    print(f\"Reduce Error : {e}\")  #reduce 빈 연산 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836732b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4224c39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.fold(0,lambda a,b : a+b) #fold 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce(), fold() 비슷한 처리를 합니다.\n",
    "\n",
    "1. reduce()는 단순한 RDD 축약연산, 비어있지 않은 rdd의 경우 \n",
    "2. fold()는 일반적이고 오류가 없는 연산, 초기값을 적용해서 비어있는 RDD, 파티션의 경우에도 적용할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34ff559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'a', 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6= spark.parallelize([1,\"a\",2])\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dcd82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    rdd6.reduce(lambda x,y : str(x)+str(y))\n",
    "except TypeError as e:\n",
    "    print(f\"Reduce Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ff9f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 13:10:56 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 34)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3192/3641956346.py\", line 2, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/05 13:10:56 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 34) (ip-172-31-1-176.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3192/3641956346.py\", line 2, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/12/05 13:10:56 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Py4JJavaError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrdd6\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:997\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m--> 997\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:949\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 949\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 34) (ip-172-31-1-176.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n    yield reduce(f, iterator, initial)\n  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3192/3641956346.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n    yield reduce(f, iterator, initial)\n  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3192/3641956346.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m      2\u001b[0m     rdd6\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m x,y : x\u001b[38;5;241m+\u001b[39my)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mPy4JJavaError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduce Error : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Py4JJavaError' is not defined"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    rdd6.reduce(lambda x,y : x+y)\n",
    "except Py4JJavaError as e:\n",
    "    print(f\"Reduce Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e57786d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py4j.protoco1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 위 str로 값 안나옴 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotoco1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m      4\u001b[0m     res\u001b[38;5;241m=\u001b[39mrdd6\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m x,y: \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(y))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'py4j.protoco1'"
     ]
    }
   ],
   "source": [
    "# 위 str로 값 안나옴 \n",
    "from py4j.protocol import Py4JJavaError\n",
    "try: \n",
    "    res=rdd6.reduce(lambda x,y: str(x)+str(y))\n",
    "    print(type(res),res)\n",
    "    \n",
    "except Py4JJavaError as e:\n",
    "    print(f'reduce 에러가 나네요 : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8d6f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1a2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.fold(\"\", lambda x,y: str(x)+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupBy, sggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ecf587a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 3, 5, 5, 8]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7= spark.parallelize([1,2,1,1,3,5,5,8])\n",
    "rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a847b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd7.groupBy(lambda x: x%2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f52131c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 1, 3, 5, 5])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x,sorted(y)) for (x,y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81589f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mRDD\u001b[49m\u001b[38;5;241m.\u001b[39maggregate(zeroValue, func1_partion, func2_combine)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RDD' is not defined"
     ]
    }
   ],
   "source": [
    "#실행 x\n",
    "RDD.aggregate(zeroValue, func1_partition, func2_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "329c4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "func1= lambda x,y : (x[0]+y, x[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4618d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "func2= lambda x,y :(x[0]+y[0], x[1]+y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e972ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd8 = spark.parallelize( [1,2,3,4],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02b56403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.aggregate( (0,0), func1, func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad520c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.glom().collect() #값 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d9bea",
   "metadata": {},
   "source": [
    "###위 시뮬레이션 \n",
    "p1\n",
    "x=[1,2], zerovalue=[0,0]\n",
    "x[0]=0, x[1]=0\n",
    "x[0]+y = 0+1=1, x[1]+1 = 0+1=1  >> (1,1)\n",
    "x[0]+y = 1+2=3, x[1]+1 = 1+1=2 >> (3,2)\n",
    "p2\n",
    "x=[3,4], zerovalue[0,0]\n",
    "x[0]=0, x[1]=0\n",
    "x[0]+y= 0+3=3, x[1]+1= 0+1=1 >> (3,1)\n",
    "x[0]+y= 3+4=7, x[1]+1= 1+1=2 >> (\n",
    "p1,p2 \n",
    "(3,2), (3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f6d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d622455",
   "metadata": {},
   "outputs": [],
   "source": [
    "## key-value RDD (paired RDD)\n",
    "\n",
    "groupByKey(), reduceByKey()\n",
    "\n",
    "그룹핑 한 후에 특정 트랜스포메이션을 수행, 키가 필요함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05d25861",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9 =spark.parallelize(\n",
    "    [\n",
    "        (\"짜장면\",15),\n",
    "        (\"짬뽕\",10),\n",
    "        (\"짜장면\",5),\n",
    "        (\"짬뽕\",20)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d8f8d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9_g= rdd9.groupByKey() #rdd를 새로 만드는 것 \n",
    "rdd9_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f2d459a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 2), ('짬뽕', 2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#집계형 연산들 \n",
    "rdd9_g.mapValues(len).collect() #max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74f5c8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', [15, 5]), ('짬뽕', [10, 20])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9_g.mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64e0da3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 10.0), ('짬뽕', 15.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9_g.mapValues(lambda x:sum(x)/ len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a267413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 20), ('짬뽕', 30)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.reduceByKey(add).collect() # 바로 연산 적용 가능 , 두단계를 하나로 묶어서 적용해줌/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ad4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduceByKey() > groupByKey () 더 빠름 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#countByKey()\n",
    "각 키별로 요소들의 갯수를 카운트 -> action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "593a113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'짜장면': 2, '짬뽕': 2})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.countByKey() #action 이라 결과가 바로 나옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e9e2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key( ) 키만 가진 RDD -> Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "332a16f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '짬뽕', '짜장면', '짬뽕']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.keys(). collect()#결과가 나오려면 action 함수 붙여줘야 함 collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f674f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join() key를 이용한 결합- outer join, inner join \n",
    "\n",
    "inner - join :(교집합) 두개의 집합에서 함께 존재하는 요소(key)의 집합 (default 기본값)\n",
    "outer-  join : 한쪽에있고, 다른쪽에 없을 때 = left, right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52a2ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1 = spark.parallelize([\n",
    "    (\"a\",10),\n",
    "    (\"b\",20),\n",
    "    (\"c\",30)\n",
    "])\n",
    "join_rdd2 = spark.parallelize([\n",
    "     (\"a\",20),\n",
    "     (\"b\",20),\n",
    "     (\"b\",10),\n",
    "    \n",
    "        ])\n",
    "join_rdd1.join(join_rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d856c533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('c', (30, None)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.leftOuterJoin(join_rdd2).collect() #왼쪽을 기준으로join_rdd1 , 대문자 주의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a75e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.rightOuterJoin(join_rdd2).collect()  #오른쪽을 기준 join_rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf34b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d17dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
